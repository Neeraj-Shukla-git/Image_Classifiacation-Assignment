{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: What is a Convolutional Neural Network (CNN), and how does it differ from traditional fully connected neural networks in terms of architecture and performance on image data?\n",
        "- A Convolutional Neural Network (CNN) is a class of deep neural networks designed to automatically and adaptively learn spatial hierarchies of features from input images. Key components are:\n",
        "\n",
        "  - Convolutional layers: apply learned kernels (filters) that convolve across spatial dimensions, producing feature maps that capture local patterns (edges, textures).\n",
        "\n",
        "  - Pooling layers: downsample spatial resolution (max/average pooling) for translation invariance and dimensionality reduction.\n",
        "\n",
        "  - Activation functions: non-linearities such as ReLU.\n",
        "\n",
        "  - Fully connected (dense) layers: typically placed near the output to perform classification from learned features.\n",
        "\n",
        "  - Batch normalization, dropout, etc. for regularization and training stability.\n",
        "\n",
        "#### Differences vs. fully connected networks (FCNs):\n",
        "\n",
        "- Parameter sharing: Convolutional kernels are reused across spatial locations → far fewer parameters than FCNs for images. FCNs need parameters proportional to input size x hidden units, which explodes for images.\n",
        "\n",
        "- Local connectivity: CNNs connect neurons to local patches (receptive fields), capturing spatial locality; FCNs flatten the image and lose spatial relationships.\n",
        "\n",
        "- Translation equivariance / invariance: Convolutions plus pooling mean that features are recognized regardless of position; FCNs lack this property.\n",
        "\n",
        "- Better performance on images: CNNs exploit structure of images, leading to much better accuracy and faster training for vision tasks.\n",
        "\n",
        "- Computational efficiency: Due to sparse local connections and weight sharing, CNNs are computationally tractable on high-dim images.\n",
        "\n",
        "- Summary: CNNs are architecturally specialized to handle grid-like data (images), producing strong generalization and efficiency advantages over plain FCNs on vision tasks.\n",
        "\n",
        "## Question 2:Discuss the architecture of LeNet-5 and explain how it laid the foundation for modern deep learning models in computer vision. Include references to its original research paper.\n",
        "\n",
        "- LeNet-5 (1998, Yann LeCun et al.) is one of the earliest successful CNNs, built for handwritten digit recognition (MNIST). Core architecture:\n",
        " - Input: 32×32 grayscale image.\n",
        " - C1: Convolutional layer with 6 filters, kernel 5×5 → output 28×28×6.\n",
        " - S2: Average pooling/subsampling (2×2) → output 14×14×6.\n",
        " - C3: Convolutional layer with 16 filters (5×5) → output 10×10×16. (Connections between C1 and C3 were not fully dense).\n",
        " - S4: Subsampling 2×2 → output 5×5×16.\n",
        " - C5: Convolutional (5×5) producing 1×1×120 (acts like fully connected).\n",
        " - F6: Fully connected layer with 84 units.\n",
        " - Output: 10-way softmax.\n",
        "- Foundational ideas introduced:\n",
        " - Local receptive fields and hierarchical feature extraction.\n",
        " - Weight sharing (convolutions) to reduce parameters.\n",
        " - Pooling/subsampling to reduce resolution and build invariance.\n",
        " - End-to-end training with backpropagation on convolutional layers.\n",
        "\n",
        "##### Why it mattered:\n",
        "- LeNet-5 provided a practical, trainable architecture that exploited image structure. Its design principles (conv → pool → conv → pool → dense) remain central to modern CNNs (AlexNet, VGG, ResNet). The original paper: “Gradient-based learning applied to document recognition” by LeCun et al., 1998 (commonly cited).\n",
        "\n",
        "## Question 3: Compare and contrast AlexNet and VGGNet in terms of design principles, number of parameters, and performance. Highlight key innovations and limitations.\n",
        "\n",
        "- AlexNet (2012):\n",
        " - Architecture: 5 convolutional layers + 3 dense layers; used ReLU, local response normalization (LRN), overlapping max-pooling.\n",
        " - Key innovations: demonstration that deep CNNs trained on GPUs achieve state-of-the-art on ImageNet; introduced ReLU for faster training; used dropout for regularization and data augmentation.\n",
        " - Parameters: ~60–100 million (depends on exact variant).\n",
        " - Performance: Won ImageNet 2012 by large margin; revived interest in deep learning.\n",
        " - Limitations: Large number of parameters, early-stage techniques (LRN not used later), relatively large filters (11×11 first layer).\n",
        "\n",
        "- VGGNet (2014):\n",
        " - Architecture: Very deep (VGG16 / VGG19) made of stacked 3×3 convolutional layers and periodic max-pooling, ended with three dense layers.\n",
        " - Key idea: Replace large filters by stacks of smaller 3×3 filters → same receptive field but fewer parameters and more non-linearities (better representational power).\n",
        " - Parameters: VGG16 ≈ 138 million (heavy due to FC layers).\n",
        " - Performance: Top performance on ImageNet for its time; demonstrated depth matters.\n",
        " - Limitations: Very heavy (storage/compute), slow inference; brittle to parameter count and memory.\n",
        "\n",
        "#### Comparison summary:\n",
        "\n",
        "- Both advanced deep-learning vision. AlexNet showed the power of deep conv nets with GPUs; VGG emphasized depth and simplicity (3×3 conv stacks).\n",
        "\n",
        "- VGG typically achieves higher accuracy than AlexNet for same training pipeline, but at higher parameter cost.\n",
        "\n",
        "- Later architectures (ResNet, Inception) improved accuracy while being more parameter-efficient and easier to train.\n",
        "\n",
        "### Question 4: What is transfer learning in the context of image classification? Explain how it helps in reducing computational costs and improving model performance with limited data.\n",
        "\n",
        "- Transfer learning: reuse of a model (or its parameters) trained on a source task/dataset (e.g., ImageNet) as the starting point for a target task. In image classification, a pre-trained CNN’s early and intermediate layers capture general visual features (edges, textures) that are often useful for other vision tasks.\n",
        "\n",
        "#### Common approaches:\n",
        "- Feature extraction: freeze pretrained convolutional base; train only new top classifier layers on target dataset.\n",
        "- Fine-tuning: unfreeze some top convolutional layers and jointly train them with the new classifier, using a smaller learning rate.\n",
        "\n",
        "#### Why it helps:\n",
        "- Reduces computational cost: training only top layers or fine-tuning a small subset requires less compute and time than training from scratch.\n",
        "- Better performance with limited data: pretrained features act as strong priors, preventing overfitting and improving generalization when labeled target data is scarce.\n",
        "- Faster convergence: model starts close to a useful solution, requiring fewer epochs to reach good performance.\n",
        "\n",
        "#### When to use:\n",
        "- When target dataset is small or similar in domain to the source (e.g., natural images).\n",
        "- Fine-tune when you have moderate data; use pure feature extraction when data is very scarce.\n",
        "\n",
        "## Question 5 Describe the role of residual connections in ResNet architecture. How do they address the vanishing gradient problem in deep CNNs?\n",
        "- Residual connections (skip connections) add the input of a block to its output: if a block computes F(x), the block's output becomes y = F(x) + x (optionally followed by activation). This simple identity shortcut allows the network to learn residual mappings.\n",
        "\n",
        "#### Why they help:\n",
        "- Easier optimization: learning F(x) = H(x) - x (residual) is often easier than directly learning H(x). If an extra layer hurts performance, the residual block can learn F(x) = 0 thereby preserving identity mapping.\n",
        "- Gradient flow: gradients can propagate directly through the identity path back to earlier layers, mitigating vanishing gradients and enabling much deeper networks (ResNet-50/101/152).\n",
        "- Better generalization and accuracy: deeper ResNets outperform shallower networks by allowing effective training of hundreds of layers.\n",
        "- Summary: Residual connections provide direct pathways for information and gradients, stabilizing training of very deep networks and reducing degradation problems."
      ],
      "metadata": {
        "id": "33C5Bo73qEze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6 Implement LeNet-5 using TensorFlow/Keras to classify the MNIST dataset. Report accuracy and training time.\n",
        "- Notes: The script below will:\n",
        "- Load MNIST, preprocess, create LeNet-5-like model\n",
        "- Train and print training time, validation accuracy\n",
        "- Save final model and history"
      ],
      "metadata": {
        "id": "lHRq78d1s4Ku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0woVb0O8pNax"
      },
      "outputs": [],
      "source": [
        "# Q6_lenet_mnist.py\n",
        "# Run: python Q6_lenet_mnist.py  OR run in Colab\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import time\n",
        "\n",
        "# 1. Load and preprocess MNIST\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# LeNet originally used 32x32; we'll resize to 32x32 and normalize\n",
        "import numpy as np\n",
        "x_train = np.expand_dims(x_train, axis=-1).astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, axis=-1).astype(\"float32\") / 255.0\n",
        "\n",
        "# Resize to 32x32\n",
        "x_train = tf.image.resize(x_train, [32, 32]).numpy()\n",
        "x_test = tf.image.resize(x_test, [32, 32]).numpy()\n",
        "\n",
        "# One-hot\n",
        "num_classes = 10\n",
        "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# 2. LeNet-5 like model\n",
        "def LeNet5(input_shape=(32,32,1), num_classes=10):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(6, kernel_size=5, activation='tanh', input_shape=input_shape, padding='valid'),\n",
        "        layers.AveragePooling2D(),  # S2\n",
        "        layers.Conv2D(16, kernel_size=5, activation='tanh', padding='valid'),\n",
        "        layers.AveragePooling2D(),  # S4\n",
        "        layers.Conv2D(120, kernel_size=5, activation='tanh'),  # C5\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(84, activation='tanh'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = LeNet5()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 3. Train\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(x_train, y_train_cat, batch_size=batch_size, epochs=epochs,\n",
        "                    validation_split=0.1, verbose=2)\n",
        "end = time.time()\n",
        "\n",
        "train_time = end - start\n",
        "print(f\"Training time (s): {train_time:.2f}\")\n",
        "\n",
        "# 4. Evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save model and history\n",
        "model.save(\"lenet_mnist.h5\")\n",
        "import json\n",
        "with open(\"lenet_mnist_history.json\", \"w\") as f:\n",
        "    json.dump({k: [float(x) for x in v] for k,v in history.history.items()}, f)\n",
        "\n",
        "# How to report after running:\n",
        "\n",
        "# Training time printed (seconds). Report as “Training time: X seconds (on GPU/CPU)”.\n",
        "\n",
        "# Test accuracy printed (e.g., 0.99). Report the final accuracy and validation curve (plot history)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7: Use a pre-trained VGG16 model via transfer learning on a small custom dataset (flowers). Replace top layers and fine-tune.\n",
        "- Notes: This script downloads the TensorFlow flower_photos dataset, prepares train/val/test splits, builds a VGG16-based classifier, and fine-tunes the top layers. It prints training time and final accuracy."
      ],
      "metadata": {
        "id": "A_QOfPkhtpJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7_vgg16_flowers.py\n",
        "# Run in Colab (GPU recommended)\n",
        "import tensorflow as tf, time, os\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Download flower dataset (TensorFlow example)\n",
        "DATA_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "dataset_dir = tf.keras.utils.get_file(origin=DATA_URL, fname=\"flower_photos\", untar=True)\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset_dir), \"flower_photos\")\n",
        "\n",
        "# 2. Create training and validation datasets\n",
        "img_size = (224, 224)  # VGG16 expects 224x224\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_dir, validation_split=0.2, subset=\"training\", seed=123,\n",
        "    image_size=img_size, batch_size=batch_size\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_dir, validation_split=0.2, subset=\"validation\", seed=123,\n",
        "    image_size=img_size, batch_size=batch_size\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# Prefetch\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# 3. Build model: VGG16 base\n",
        "base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet',\n",
        "                                         input_shape=img_size+(3,))\n",
        "base_model.trainable = False  # freeze base\n",
        "\n",
        "# Add top classifier\n",
        "inputs = tf.keras.Input(shape=img_size+(3,))\n",
        "x = tf.keras.applications.vgg16.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 4. Train top layers\n",
        "epochs_head = 8\n",
        "start = time.time()\n",
        "history1 = model.fit(train_ds, epochs=epochs_head, validation_data=val_ds)\n",
        "end = time.time()\n",
        "print(\"Feature-extract training time (s):\", end-start)\n",
        "\n",
        "# 5. Fine-tune: unfreeze some top VGG blocks\n",
        "base_model.trainable = True\n",
        "# Freeze lower layers, unfreeze last convolutional block\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "history2 = model.fit(train_ds, epochs=5, validation_data=val_ds)\n",
        "end = time.time()\n",
        "print(\"Fine-tuning time (s):\", end-start)\n",
        "\n",
        "# 6. Save and report\n",
        "model.save(\"vgg16_flowers_finetuned.h5\")\n",
        "# Evaluate on validation set (optionally create test split)\n",
        "val_loss, val_acc = model.evaluate(val_ds)\n",
        "print(f\"Validation accuracy after fine-tuning: {val_acc:.4f}\")\n",
        "\n",
        "# Plot training history (optional)\n",
        "acc = history1.history['accuracy'] + history2.history['accuracy']\n",
        "val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
        "plt.plot(acc, label='train_acc'); plt.plot(val_acc, label='val_acc'); plt.legend(); plt.show()\n",
        "\n",
        "\n",
        "# Discussion points to include in the assignment PDF after running:\n",
        "\n",
        "# Number of classes and dataset size.\n",
        "\n",
        "# Training times for feature-extract stage and fine-tuning stage (both printed).\n",
        "\n",
        "# Final validation accuracy and a short analysis: whether model overfits, whether more augmentation is needed, etc.\n"
      ],
      "metadata": {
        "id": "2eJ4WYMStXlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8: Visualize the filters and feature maps of the first convolutional layer of AlexNet on an example input image.\n",
        "- Notes: We'll implement an AlexNet-like small model in Keras (first conv layer similar to AlexNet) and visualize:\n",
        " - The learned filters (weights) of the first Conv2D layer\n",
        " - The feature maps (activations) after feeding a sample image"
      ],
      "metadata": {
        "id": "PAJUvfFlt6mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8_alexnet_filters_featuremaps.py\n",
        "# Run in Colab or local\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "\n",
        "# 1. Define small AlexNet-ish model (only first few layers needed)\n",
        "def AlexNet_small(input_shape=(227,227,3), num_classes=1000):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(96, kernel_size=11, strides=4, activation='relu')(inputs)  # first layer like AlexNet\n",
        "    x = layers.MaxPooling2D(pool_size=3, strides=2)(x)\n",
        "    x = layers.Conv2D(256, kernel_size=5, padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=3, strides=2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(4096, activation='relu')(x)\n",
        "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return models.Model(inputs, x)\n",
        "\n",
        "model = AlexNet_small(input_shape=(227,227,3), num_classes=10)\n",
        "model.summary()\n",
        "\n",
        "# 2. Randomly initialize or load weights (we'll use random weights here)\n",
        "# To visualize filters that are meaningful, you would train this model on a dataset.\n",
        "# But you can still visualize initial filters.\n",
        "\n",
        "# 3. Visualize filters of first conv layer\n",
        "first_conv = model.layers[1]  # layer index for first Conv2D\n",
        "weights = first_conv.get_weights()[0]  # shape (11,11,3,96)\n",
        "print(\"weights shape:\", weights.shape)\n",
        "\n",
        "# Normalize filter values to 0-1 for visualization\n",
        "w_min, w_max = weights.min(), weights.max()\n",
        "filters = (weights - w_min) / (w_max - w_min)\n",
        "\n",
        "n_filters = min(16, filters.shape[3])\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "for i in range(n_filters):\n",
        "    f = filters[:, :, :, i]\n",
        "    # resize for display\n",
        "    ax = fig.add_subplot(2, n_filters//2, i+1)\n",
        "    ax.imshow(f)\n",
        "    ax.axis('off')\n",
        "plt.suptitle(\"First-layer filters (initial random weights)\")\n",
        "plt.show()\n",
        "\n",
        "# 4. Feature maps on one sample image\n",
        "# Use a sample image from keras or local path\n",
        "img_path = tf.keras.utils.get_file('elephant.jpg',\n",
        "                                   'https://upload.wikimedia.org/wikipedia/commons/7/73/Loxodonta_africana_%28cropped%29.jpg')\n",
        "img = image.load_img(img_path, target_size=(227,227))\n",
        "img_arr = image.img_to_array(img)\n",
        "img_input = np.expand_dims(img_arr, axis=0) / 255.0\n",
        "\n",
        "# Create a model that outputs activations of first conv layer\n",
        "activation_model = models.Model(inputs=model.input, outputs=first_conv.output)\n",
        "activations = activation_model.predict(img_input)  # shape (1, out_h, out_w, 96)\n",
        "print(\"activations shape:\", activations.shape)\n",
        "\n",
        "# Plot some feature maps\n",
        "n_maps = 16\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "for i in range(n_maps):\n",
        "    ax = fig.add_subplot(2, n_maps//2, i+1)\n",
        "    act = activations[0, :, :, i]\n",
        "    ax.imshow(act, cmap='viridis')\n",
        "    ax.axis('off')\n",
        "plt.suptitle(\"Feature maps from first conv layer (random weights)\")\n",
        "plt.show()\n",
        "\n",
        "# Note: With random weights the filters/feature maps are not semantically meaningful. After training on real data, the filter visualizations show edges, color blobs, etc."
      ],
      "metadata": {
        "id": "Co_W6efsuEvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Train an Inception variant (InceptionV3) on CIFAR-10. Plot training/validation accuracy over epochs and analyze overfitting or underfitting.\n",
        "- Notes: InceptionV3 expects bigger input, so images are resized. This script shows training and plotting. Use GPU for reasonable speed."
      ],
      "metadata": {
        "id": "eeZ2KlcRuO77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9_inceptionv3_cifar10.py\n",
        "import tensorflow as tf, time\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "num_classes = 10\n",
        "\n",
        "# Resize to 150x150 for InceptionV3\n",
        "IMG_SIZE = 150\n",
        "x_train = tf.image.resize(x_train, [IMG_SIZE, IMG_SIZE]).numpy().astype('float32')/255.0\n",
        "x_test = tf.image.resize(x_test, [IMG_SIZE, IMG_SIZE]).numpy().astype('float32')/255.0\n",
        "\n",
        "# 2. Build model with InceptionV3 base\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet',\n",
        "                                               input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = tf.keras.applications.inception_v3.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 3. Train\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(x_train, y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=2)\n",
        "end = time.time()\n",
        "print(f\"Training time (s): {end-start:.2f}\")\n",
        "\n",
        "# 4. Plot accuracy\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.title('InceptionV3 on CIFAR-10 (feature-extract)')\n",
        "plt.show()\n",
        "\n",
        "# 5. Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# 6. (Optional) Fine-tune top base layers for more accuracy (unfreeze some top layers)\n",
        "\n",
        "# Analysis you should include in your PDF after running:\n",
        "\n",
        "# Plot: training vs validation accuracy — if validation gap large (val << train) → overfitting (suggest augmentation, regularization).\n",
        "\n",
        "# If both low → underfitting (suggest larger model, longer training, reduce regularization).\n",
        "\n",
        "# Report training time and final test accuracy."
      ],
      "metadata": {
        "id": "-n1yRR4_uVgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10 You are working in a healthcare AI startup. Task: classify X-ray images into normal, pneumonia, and COVID-19 with limited labeled data. Which approach and deployment strategy?\n",
        "\n",
        "Answer (recommended approach + code snippet + deployment plan)\n",
        "\n",
        "Recommended approach\n",
        "\n",
        "Use transfer learning with a robust backbone: e.g., ResNet50 or EfficientNet-B0/B3 pretrained on ImageNet. These backbones extract general features and are proven effective for medical imagery when fine-tuned.\n",
        "\n",
        "Data handling\n",
        "\n",
        "Gather all labeled X-rays (3 classes). If extremely limited, augment with domain-similar public datasets (e.g., NIH ChestX-ray, RSNA pneumonia dataset, COVIDx) after ensuring licensing and ethical use.\n",
        "\n",
        "Data augmentation (careful): rotations, shifts, slight brightness/contrast; avoid unrealistic transforms that change clinical features.\n",
        "\n",
        "Class imbalance: use class weighting, oversampling, or focal loss.\n",
        "\n",
        "Training strategy\n",
        "\n",
        "Stage 1: Freeze the base model, train only top classifier head (feature extraction).\n",
        "\n",
        "Stage 2: Unfreeze top few convolutional blocks and fine-tune with a low learning rate.\n",
        "\n",
        "Use cross-validation (k-fold) if data small; report sensitivity (recall), specificity, AUC for each class — accuracy alone is insufficient in medical tasks.\n",
        "\n",
        "Regularization & calibration\n",
        "\n",
        "Use dropout, weight decay, and early stopping.\n",
        "\n",
        "Calibrate predicted probabilities (Platt scaling / temperature scaling) before deployment.\n",
        "\n",
        "Explainability\n",
        "\n",
        "Integrate Grad-CAM or other saliency maps to visualize model focus — mandatory for medical settings to support clinicians.\n",
        "\n",
        "Evaluation\n",
        "\n",
        "Use withheld test set from different source (external validation) to estimate generalization.\n",
        "\n",
        "Report confusion matrix, per-class precision/recall, F1, ROC/AUC, and NPV/PPV for clinical relevance.\n",
        "\n",
        "Deployment strategy\n",
        "\n",
        "Model packaging: export a SavedModel (TensorFlow) or ONNX.\n",
        "\n",
        "Serving: use TensorFlow Serving or a Dockerized FastAPI/Flask app with GPU inference if needed.\n",
        "\n",
        "Monitoring: log inputs, outputs, model confidence, and implement data drift detection; monitor for distributional shifts.\n",
        "\n",
        "Human-in-the-loop: allow clinician review for low-confidence cases; start as decision-support, not autonomous diagnosis.\n",
        "\n",
        "Security & compliance: HIPAA/GDPR rules, secure data handling, audit logs, version control for model and data.\n",
        "\n",
        "CI/CD: automated tests, canary rollout, A/B testing, rollback strategies."
      ],
      "metadata": {
        "id": "J5O3HvEguhez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10_resnet_xray.py\n",
        "import tensorflow as tf, time\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "# Assume you have train_dir, val_dir with subfolders 'normal','pneumonia','covid'\n",
        "train_dir = \"/path/to/train\"\n",
        "val_dir = \"/path/to/val\"\n",
        "img_size = (224,224)\n",
        "batch_size = 16\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_dir, image_size=img_size, batch_size=batch_size)\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(val_dir, image_size=img_size, batch_size=batch_size)\n",
        "\n",
        "# Data augmentation pipeline\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.05),\n",
        "    layers.RandomContrast(0.05),\n",
        "])\n",
        "\n",
        "base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=img_size+(3,))\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=img_size+(3,))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.resnet.preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(3, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "print(\"Feature-extract time:\", time.time()-start)\n",
        "\n",
        "# Fine-tune: unfreeze last conv block\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "start = time.time()\n",
        "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "print(\"Fine-tune time:\", time.time()-start)\n",
        "\n",
        "# Save\n",
        "model.save(\"resnet_xray_model\")\n",
        "\n",
        "\n",
        "\n",
        "# Deployment outline\n",
        "\n",
        "# Export SavedModel.\n",
        "\n",
        "# Create a Docker image using tensorflow/serving or a small FastAPI wrapper that loads the model and exposes /predict.\n",
        "\n",
        "# Use TLS, authentication, and write audit logs.\n",
        "\n",
        "# Set up monitoring dashboards (latency, throughput, prediction distributions).\n",
        "\n",
        "# Conduct clinician validation and pilot deployment in controlled clinical setting before large-scale roll-out.\n",
        "\n",
        "# Ethics & safety: Ensure annotated data quality, obtain clinician sign-off, and be cautious: models are decision-support tools, not replacements for clinical judgement."
      ],
      "metadata": {
        "id": "iYiQfFXVutHQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}